## Textures
Textures in WebGL serve as a fundamental mechanism for adding detail and visual richness to 3D graphics. At their core, textures are simply arrays of data that can be sampled during rendering. While most commonly used for mapping images onto surfaces, textures can actually store various types of data including normal maps, height maps, and even non-visual data used in computational shaders.  
The texture mapping process begins with texture coordinates, often called UV coordinates. These coordinates provide a mapping between points on your 3D geometry and positions within the texture. UV coordinates are typically normalized between 0 and 1, where (0,0) represents the bottom-left corner of the texture and (1,1) represents the top-right corner. This normalization allows the same texture coordinates to work with textures of any resolution.  
**Texture sampling** is the process of reading values from a texture at specific UV coordinates. WebGL provides built-in functions like texture2D() in GLSL that handle the complex process of determining which texel (texture pixel) values to read and how to interpolate between them. This sampling process is guided by texture parameters that control behavior like filtering and wrapping.
**Texture filtering** determines how the texture appears when viewed at different scales. When a textured surface is viewed from far away, multiple texels may map to a single pixel on screen - this is called minification. Conversely, when viewed very close up, a single texel may cover multiple screen pixels - this is called magnification. WebGL provides different filtering options for each case. Linear filtering smoothly blends between texels, while nearest filtering selects the closest texel, creating a pixelated look.  
Memory management becomes critical when working with textures in WebGL. Textures consume significant GPU memory, especially at high resolutions. WebGL implementations typically have limits on both the maximum texture size and the total texture memory available. Techniques like texture atlasing (packing multiple smaller textures into one larger texture) and texture compression can help manage these constraints.
WebGL's texture capabilities extend beyond simple image mapping. Render-to-texture techniques allow rendering scenes to textures instead of the screen, enabling effects like reflections, shadow maps, and post-processing. Multiple textures can be used together in a single shader, allowing techniques like normal mapping, where surface detail is simulated by perturbing surface normals based on a texture.